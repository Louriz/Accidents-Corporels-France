{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#required libraries : \n",
    "import pandas as pd  # dataframe structure\n",
    "import numpy as np  # array structure\n",
    "import seaborn as sns # visualization\n",
    "import matplotlib.pyplot as plt # plot\n",
    "import glob, os  # dealing with OS and Files, reading files etc \n",
    "import time\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, explained_variance_score\n",
    "import glob, os  # dealing with OS and Files, reading files etc \n",
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "import pickle # saving model, data, etc : serialization\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Risque de fréquence d'accidents et le risk de gravité associé "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problématique : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce notebook nous allons nous intéresser à étudier seulement l'impact de la configuration ou la description d'un lieu d'accident sur la fréquence et la gravité des accidents. \n",
    "\n",
    "Donc les objectifs sont :\n",
    "    1. de pouvoir élaborer un risque de fréquence d'accidents  en fonction d'une configuration donnée du lieu d'accident\n",
    "    2. de pouvoir élaborer un risque de gravité d'accidents en fonction d'une configuration donnée du lieu d'accident\n",
    "    \n",
    "L'ONISR peut déployer cette solution dans le but de prédire les risques associés à un nouvel aménagement ou trouver les configurations avec plus d'accidents et les aménager."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formalisation en Data Mining : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contraintes :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant de commencer la traduction de la problématique en problème fouille de données, nous devons d'abord définir la variable cible. Pour cela nous allons créer lors de la phase de préparation des données deux variables :\n",
    "    1. Une variable qui va considérer le nombre d'accidents par configuration de route, puis discrétiser cette variable en deux niveaux\n",
    "    2. Une variable qui va considérer la  gravité des personnes impliqueés par configuration de route, puis discrétiser cette variable en deux niveaux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formalisation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectif 1 : Un modèle de classification -en utilisant les données à disposition- afin de calculer le risque associé à une configuration d'une route donnée.\n",
    "\n",
    "Objectif 2 : Un modèle de classification -en utilisant les données à disposition- afin de calculer le risque de gravité d'une route donnée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Méthode de travail : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La méthode qui sera utilisé tout au long du projet est la méthode la plus populaire CRISP-DM dont le cycle de vie est présenté ci-dessus : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](crisp.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vu que nos objectifs nécessaitent les mêmes données, la même préparation et le même nettoyage nous allons suivre la méthode CRISP-DM pour les deux objectifs parallèlement. Avant de se lancer dans la compréhension du marché, nous allons élaborer d'abord notre plan de projet : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table=[[\"Phase\",\t\"Temps\",\"Ressources\",\"Risques\"],\n",
    "       [\"Compréhension du problème\", \"1 semaine\",\"tout le groupe\",\"Changement économique\"],\n",
    "[\"Compréhension des données\",\"1 semaines\",\"tout le groupe\", \"Problèmes de données, problèmes technologiques\"],\n",
    "       [\"Préparation des données\",\"3 semaines\",\"tout le groupe\",\"Problèmes de données, problèmes technologiques\"],\n",
    "[\"Modélisation\" ,\"1 semaines\", \"tout le groupe\",\"Problèmes technologiques, incapacité à trouver un modèle adéquat\"],\n",
    "[\"Evaluation\", \"1 semaine\", \"Encadrants, Hugo DARCHIS et Edouard LE COZ\", \"incapacité à mettre en oeuvre les résultats\"],\n",
    "[\"Déploiement\",\"1 semaine\",\" Riahi LOURIZ\",\"Changement économique, incapacité à mettre en oeuvre les résultats\"]]\n",
    "display(HTML(tabulate.tabulate(table, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compréhension du marché "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reste à faire...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compréhension des données \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette implique l'étude des données à disposition sur les accidents corporels en France. Cette étape a un poids important dans la méthode CRISP-DM car elle permet d'éviter les problèmes inattendues au cours de la phase de la préparation des données. Pour bien amener cette étape, nous allons suivre les étapes suivantes : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A : Collecte des données initiales: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Sources des données :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données utilisées afin de répondre à nos problématiques ont été collectées depuis la plateforme ouvertes des données publiques françaises (Datagov). Ces données sont saisies et rassemblées dans une fiche intitulée bulletin d’analyse des accidents corporels. L’ensemble de ces fiches forme le fichier national des accidents corporels de la circulation dit Fichier BAAC administré par l’Observatoire national interministériel de la sécurité routière (ONISR).\n",
    "Les données sont fournies pour chaque année ( de 2005 à 2016) sous forme de\n",
    "quatre tables :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. La table <b>CARACTERISTIQUES</b> qui décrit les circonstances générales de\n",
    "l’accident\n",
    "2. La table <b>LIEUX</b> qui décrit le lieu principal de l’accident même si celui-ci s’est déroulé à une Intersection\n",
    "3. La table <b>VEHICULES</b> impliqués\n",
    "4. La table <b>USAGERS</b> impliqués"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour la définition des attributs de chaque table voir document PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Attributs prometteurs : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En se basant sur l'objectif fixé au début du notebook, nous allons lister les variables les plus intérresantes pour mener le reste de l'étude. Vu qu'on est intéressé sur l'impact de la configuration ou la description d'une route sur la fréquence et la gravité des accidents, alors nous allons choisir les variables suivantes (pour la siginification des variables veuillez référer au document PDF): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_used=[ 'agg', 'int', 'catr', 'circ', 'nbv', 'vosp','prof', 'plan', 'lartpc','larrout', 'infra','situ', 'obs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Fusion des données :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les quatres tables  pour la période (2005-2016) doivent être rassemblés sur quatres tables : \n",
    "1. La table <b>CARACTERISTIQUES</b> sur toute la période 2005-2016\n",
    "2. La table <b>LIEUX</b>  sur toute la période 2005-2016\n",
    "3. La table <b>VEHICULES</b>  sur toute la période 2005-2016\n",
    "4. La table <b>USAGERS</b>  sur toute la période 2005-2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path =r'dataset/caracteristiques' # path containing all csv files about caracteristiques (  use your own path)\n",
    "allFiles = glob.glob(path + \"/*.csv\")  # a list containing files names (all csv files)\n",
    "list_ = []\n",
    "for file_ in allFiles:\n",
    "    if '2009' in file_ :\n",
    "        df = pd.read_table(file_,encoding='latin-1') # read table file (file_) for year 2009\n",
    "        list_.append(df)  # append the the dataframe df to the lust list_\n",
    "    else : \n",
    "        df = pd.read_csv(file_,encoding='latin-1') # read csv file (file_)\n",
    "        list_.append(df)  # append the the dataframe df to the lust list_     \n",
    "        \n",
    "caracteristiques = pd.concat(list_)  # concat all dataframes existing in list_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path =r'dataset/usagers' \n",
    "allFiles = glob.glob(path + \"/*.csv\")\n",
    "frame = pd.DataFrame()\n",
    "list_ = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,encoding='latin-1')\n",
    "    list_.append(df)\n",
    "usagers = pd.concat(list_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r'dataset/lieux' # use your path\n",
    "allFiles = glob.glob(path + \"/*.csv\")\n",
    "frame = pd.DataFrame()\n",
    "list_ = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,encoding='latin-1')\n",
    "    list_.append(df)\n",
    "lieux = pd.concat(list_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path =r'dataset/vehicules' # use your path\n",
    "allFiles = glob.glob(path + \"/*.csv\")\n",
    "frame = pd.DataFrame()\n",
    "list_ = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,encoding='latin-1')\n",
    "    list_.append(df)\n",
    "vehicules = pd.concat(list_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Taille de données et contrainte pour les problématiques: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La base de données contient un historique à partir de 2005 jusqu'au 2016, par conséquent on a suffisamment des données pour traiter nos problématiques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### B. Description des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il exist plusieurs méthodes pour décrire les données, mais la plupart des descriptions sont axées sur la quantité et la qualité des données : le volume de données disponibles et l'état de ces données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Quantité de données:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory size for each table : \n",
    "print(\"Info abour caracteristiques : \")\n",
    "caracteristiques.info()\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"Info abour lieux :\") \n",
    "lieux.info()\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"Info abour vehicules :\")\n",
    "vehicules.info()\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"Info abour usagers :\") \n",
    "usagers.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-----------Shape of our dataframes ---------------\")\n",
    "print(\"The shape of caracteristiques is :\", caracteristiques.shape)\n",
    "print(\"The shape of lieux is :\", lieux.shape)\n",
    "print(\"The shape of vehicules is :\", vehicules.shape)\n",
    "print(\"The shape of usagers is :\", usagers.shape)\n",
    "print(\"Total Missing Data (note that 0 means also a nan value but should be converted to nan later during cleaning stage)\")\n",
    "print(\"Total missing data in  caracteristiques is :\", caracteristiques.isnull().sum().sum())\n",
    "print(\"Total missing data in  lieux is :\", lieux.isnull().sum().sum())\n",
    "print(\"Total missing data in  vehicules is :\", vehicules.isnull().sum().sum())\n",
    "print(\"Total missing data in  usagers is :\", usagers.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Types de valeur : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D'après la partie ci-dessus, nous remarquons très bien variables se présente sous un seul type, à savoir : numérique.\n",
    "\n",
    "Certes ce type n'est pas le seul présenté dans notre jeu de données car on a beaucoup de variables catégorielles  codées numériquement avec des entiers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Méthode de codage :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "la partie ci-dessus nous permet déjà de penser aux types de codage qui sera utilisé par la suite lors de la modélisation, soit on garde ce codage ou on va utiliser le <b>Hot encoding</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### C. Vérification de la qualité des données : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données sont rarement présentées bien nettoyées. En fait, les données généralement contiennent des valeurs manquantes, des points aberrants, des erreurs de saisie etc. Le document définissant la base de données mentionne avec toute clarté qu'il aie des valeurs manquantes : \"La plupart des variables contenues dans les quatre fichiers précédemment énumérés peuvent contenir\n",
    "des cellules vides ou un zéro ou un point. Il s’agit, dans ces trois cas, d’une cellule non renseignée par\n",
    "les forces de l'ordre ou sans objet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous n'allons pas nettoyer les données à ce stade, mais cela sera une partie dans la préparation des données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### D. Exploration des données : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utiliser tableau pour visualiser les données ou simplement faire des table croisé ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Préparation des données\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D'après l'effort qui a été consacré aux étapes précédentes, nous allons lister les principales tâches à faire lors de la préparation des données : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fusion des quatres tables \n",
    "- Aggrégation des lignes selon les colonnes choisie pour la définition de la configuration d'une route.\n",
    "- Traitement des valeurs manquantes\n",
    "Les colonnes aggrégées sont : \n",
    "    1. Num_Acc (opération d'aggrégation est <b> count </b>) \n",
    "    2. grav (opération d'aggrégation est <b> value_counts</b>)\n",
    "    \n",
    "- Traitement des outliers\n",
    "- Feature engineering :  création de deux variables cibles qui vont mesurer le risk de fréquence d'accident et le score de gravité pour chaque configuration du lieu d'accident.\n",
    "- Fractionnement du jeu de données en train/test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Fusion des quatres tables : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# on merge caracteristiques et lieux puis on merge avec vehicules : \n",
    "carac_lieux_veh_merged=pd.merge(pd.merge(caracteristiques,lieux,on='Num_Acc'), vehicules,on='Num_Acc' )\n",
    "#on merge finalement avec usagers :\n",
    "data_merged=pd.merge(carac_lieux_veh_merged,usagers,on=['Num_Acc','num_veh'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show first and last 3 rows : \n",
    "data_merged.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merged.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save data_merged to csv file : \n",
    "data_merged.to_csv('score-analysis/data_merged.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Traitement des valeurs manquantes : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous n'allons imputer que les variables qui nous intéresse qui sont définie dans la liste <b>col_used</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rappel : \n",
    "print(col_used)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traiter les valeurs maquantes revient à savoir les mécanismes dérier (MAR, MCAR, MNAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une carte de chaleur de corrélation (<b>heatmap correlation</b> simple est montrée ci-dessous. Cette carte décrit le degré de relation de nullité entre les différentes caractéristiques. La plage de cette corrélation de nullité va de -1 à 1 (-1 ≤ R ≤ 1). Les variables sans valeur manquante sont exclues du heatmap. Si la corrélation de nullité est très proche de zéro (-0,05 <R <0,05) (par exemple: catr vs catr), aucune valeur ne sera affichée. En outre, une corrélation de nullité positive parfaite (R = 1) indique que la première variable et la deuxième variable ont toutes les deux des valeurs manquantes correspondantes (MAR) alors qu'une corrélation de nullité négative parfaite (R = -1) signifie que l'une des variables est manquante alors que  la seconde ne l'est pas (MNAR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.heatmap(data_merged.loc[:,col_used])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le heatmap nous permet de conclure quant à la méthode d'imputation qui doit être utilisée. Cette méthode ne doit pas être stationnaire (imputation par le mode (variables catégorielles) ou la moyenne (variables numériques) car on n'est pas face à un phénomène MCAR, et donc une supression des lignes correspondant aux valeurs manquantes baisera les résultats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cependant les données présentent des variables de différents types et les méthodes qui sont utilisées pour l'imputation des valeurs manquantes gérent mieux que les valeurs numériques comme <b>fancyimpute</b> et <b>MICE</b> (Multiple Imputation by Chained Equations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par conséquent nous allons opter pour un algorithme qui traitent les valeurs manquantes d'une façon interne. On va choisir <b>xgboost</b> qui traitent les valeurs manquantes d'une telle façon à minimiser la fonction de coût qui lui est propore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ceci nous finit pas cette partie, car dans le jeu de données il n'a pas que des nan qui impliquent une valeur manquantes mais aussi des 0 ou enore des cases vides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Aggrégation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette partie a pour objectif d'agréger les données préparées selon les attributs prometteurs. La variable à agréger est \"grave\" et \"Num Acc\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette aggrégation sera fait sur deux jeu de données : train et test.\n",
    "\n",
    "le jeu de données du test sera la région Bretagne  et le jeu de données du train sera le reste de la France."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>Split en train/test : </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_merged.long=data_merged.long.replace('-',np.nan)  # replace - into nan values\n",
    "data_merged.long=data_merged.long.astype(float)   # change type of long to be float "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normaliser les coordonnéés  :\n",
    "data_merged.long=data_merged.long/100000\n",
    "data_merged.lat=data_merged.lat/100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#bretagne :\n",
    "long_min=-4.793000\n",
    "long_max=-0.880750\n",
    "lat_max=48.998600\n",
    "lat_min=47.300030\n",
    "\n",
    "bretagne=data_merged.loc[((data_merged.long <= long_max )&(data_merged.long >= long_min)),:]\n",
    "bretagne=bretagne.loc[((bretagne.lat <=lat_max)&(bretagne.lat >= lat_min)),:]\n",
    "\n",
    "#france : \n",
    "france=data_merged.loc[~((data_merged.long<long_max)&(data_merged.long> long_min)),:]\n",
    "france=france.loc[~((france.lat<lat_max )&(france.lat> lat_min)),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#group by operation followed by value_couts upon grav variable\n",
    "#--------------------------------France---------------------------\n",
    "#group by for france data : \n",
    "france_grouped=france.groupby([ 'agg', 'int', 'catr', 'circ', 'nbv', 'vosp','prof', \n",
    "                                  'plan', 'lartpc','larrout', 'infra','situ', 'obs']).grav.value_counts()\n",
    "france_grouped = france_grouped.rename(columns={'grav': 'nb_grav'})\n",
    "#create dataframe by reseting index : \n",
    "france_grouped=france_grouped.reset_index()\n",
    "france_grouped=france_grouped.rename(columns={0:'nb_grav'})\n",
    "\n",
    "\n",
    "#-----------------------------Bretagne ----------------------------\n",
    "#group by for bretagne data : \n",
    "bretagne_grouped=bretagne.groupby([ 'agg', 'int', 'catr', 'circ', 'nbv', 'vosp','prof', \n",
    "                                  'plan', 'lartpc','larrout', 'infra','situ', 'obs']).grav.value_counts()\n",
    "bretagne_grouped = bretagne_grouped.rename(columns={'grav': 'nb_grav'}) # rename aggregated column\n",
    "#create dataframe by reseting index : \n",
    "bretagne_grouped=bretagne_grouped.reset_index()\n",
    "bretagne_grouped=bretagne_grouped.rename(columns={0:'nb_grav'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <b><mark>Création de nouveau d'attributs :</mark></b> ces attributs vont nous permettre de calculer le score de gravité d'une configuration donnée du lieu d'accident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#---------------------France ----------------------\n",
    "france_grouped['indemne']=0  # 1\n",
    "france_grouped['tue']=0  # 2 \n",
    "france_grouped['hospitalisé']=0  # 3\n",
    "france_grouped['blessé']=0  # 4 \n",
    "#----------------------Bretagbe \n",
    "bretagne_grouped['indemne']=0  # 1\n",
    "bretagne_grouped['tue']=0  # 2 \n",
    "bretagne_grouped['hospitalisé']=0  # 3\n",
    "bretagne_grouped['blessé']=0  # 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fill in new attributs : \n",
    "#------------------France ---------------------\n",
    "for i in range(france_grouped.shape[0]):\n",
    "    if france_grouped.loc[i,'grav']==1 :\n",
    "        france_grouped.loc[i,'indemne']=france_grouped.loc[i,'nb_grav']\n",
    "    elif france_grouped.loc[i,'grav']==2 :\n",
    "        france_grouped.loc[i,'tue']=france_grouped.loc[i,'nb_grav']\n",
    "    elif france_grouped.loc[i,'grav']==3 :\n",
    "        france_grouped.loc[i,'hospitalisé']=france_grouped.loc[i,'nb_grav']\n",
    "    else :\n",
    "        france_grouped.loc[i,'blessé']=france_grouped.loc[i,'nb_grav']\n",
    "        \n",
    "        \n",
    "#------------------Bretagne---------------------\n",
    "for i in range(france_grouped.shape[0]):\n",
    "    if bretagne_grouped.loc[i,'grav']==1 :\n",
    "        bretagne_grouped.loc[i,'indemne']=bretagne_grouped.loc[i,'nb_grav']\n",
    "    elif bretagne_grouped.loc[i,'grav']==2 :\n",
    "        bretagne_grouped.loc[i,'tue']=bretagne_grouped.loc[i,'nb_grav']\n",
    "    elif bretagne_grouped.loc[i,'grav']==3 :\n",
    "        bretagne_grouped.loc[i,'hospitalisé']=bretagne_grouped.loc[i,'nb_grav']\n",
    "    else :\n",
    "        bretagne_grouped.loc[i,'blessé']=bretagne_grouped.loc[i,'nb_grav']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#aggregate the last 5 columns to have one row per description of road :\n",
    "#--------------------------------------France-----------------------------------------------------------------\n",
    "df_france=france_grouped.groupby([ 'agg', 'int', 'catr', 'circ', 'nbv', 'vosp','prof', 'plan', 'lartpc',\n",
    "                   'larrout', 'infra','situ', 'obs']).agg({'indemne':['sum'],'tue':['sum'],\n",
    "                                                                  'hospitalisé':['sum'],'blessé':['sum']})\n",
    "# create data frame\n",
    "df_france=df_france.reset_index()\n",
    "#rename columns\n",
    "col_names=['agg','int','catr','circ', 'nbv', 'vosp', 'prof','plan','lartpc','larrout',\n",
    " 'infra','situ', 'obs','indemne','tue','hospitalisé','blessé']\n",
    "df_france.columns=col_names\n",
    "#---------------------------------------Bretagne---------------------------------------------------------------\n",
    "df_bretagne=bretagne_grouped.groupby([ 'agg', 'int', 'catr', 'circ', 'nbv', 'vosp','prof', 'plan', 'lartpc',\n",
    "                   'larrout', 'infra','situ', 'obs']).agg({'indemne':['sum'],'tue':['sum'],\n",
    "                                                                  'hospitalisé':['sum'],'blessé':['sum']})\n",
    "# create data frame\n",
    "df_bretagne=df_bretagne.reset_index()\n",
    "#rename columns\n",
    "col_names=['agg','int','catr','circ', 'nbv', 'vosp', 'prof','plan','lartpc','larrout',\n",
    " 'infra','situ', 'obs','indemne','tue','hospitalisé','blessé']\n",
    "df_bretagne.columns=col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#aggregate data for Num_Acc\n",
    "#-------------France ------------------------\n",
    "res_france=france.groupby(['agg', 'int', 'catr', 'circ', 'nbv', 'vosp','prof', 'plan', 'lartpc',\n",
    "                   'larrout', 'infra','situ', 'obs']).agg({'Num_Acc':['count']})\n",
    "\n",
    "#create dataframe :\n",
    "res_france=res_france.reset_index()\n",
    "#rename columns\n",
    "col_france=['agg','int','catr','circ', 'nbv', 'vosp', 'prof','plan','lartpc','larrout',\n",
    " 'infra','situ', 'obs','nb_acc']\n",
    "res_france.columns=col_france\n",
    "#-------------------bretagne\n",
    "res_bretagne=bretagne.groupby(['agg', 'int', 'catr', 'circ', 'nbv', 'vosp','prof', 'plan', 'lartpc',\n",
    "                   'larrout', 'infra','situ', 'obs']).agg({'Num_Acc':['count']})\n",
    "#create dataframe :\n",
    "res_bretagne=res_bretagne.reset_index()\n",
    "#rename columns\n",
    "res_bretagne.columns=col_france"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add nb_acc to df  to represent the seconde column that will allow us to compute risk of accident frequence\n",
    "#-------------------France ---------------\n",
    "df_france['nb_acc'] = res_france['nb_acc']\n",
    "\n",
    "#---------------Bretagne-----------------\n",
    "df_bratgne['nb_acc']=res_bretagne['nb_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save data to csv file :\n",
    "df_france.to_csv('score-analysis/df_france.csv',index=False)\n",
    "df_bretagne.to_csv('score-analysis/df_bretagne.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_france=pd.read_csv('score-analysis/df_france.csv', encoding='latin-1')\n",
    "df_bretagne=pd.read_csv('score-analysis/df_bretagne.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant créer deux variables cibles : \n",
    "    1. score d'accident: une score qui prend en considération la variable nb_acc (nombre d'accidents par configuration de route)\n",
    "    2. score de gravité : un score qui prend en considération les 4 variables 'indemne','tue','hospitalisé' et 'blessé'.\n",
    "    \n",
    "Un score élevé signifie que la configuration de la route est très dangereuse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit très bien la difficulté de créer ces score. L'objectif est de trouver pour chaque score une fonction f(.) qui va fournir ce score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>Proposition</mark> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# score d'accident : \n",
    "#-----------------France---------------------\n",
    "df_france['sc_acc'] =(df_france.nb_acc-df_france.nb_acc.min())/(df_france.nb_acc.max()-df_france.nb_acc.min())\n",
    "# score de gravité : \n",
    "df_france['sc_grav'] =( df_france['indemne'] + (df_france['blessé'] * np.exp(1)) +\n",
    "                                   (df_france['hospitalisé'] * np.exp(2)) + (df_france['tue'] * np.exp(3)))\n",
    "#normalize sc_grav\n",
    "df_france['sc_grav_norm']=(df_france.sc_grav)/(df_france.sc_grav.max())\n",
    "# create two levels for sc_grav_norm\n",
    "df_france['risk_gravity']=1\n",
    "df_france.loc[((df_france['sc_grav_norm']<=df_france.sc_grav_norm.describe()['50%'])),'risk_gravity']=1\n",
    "df_france.loc[((df_france['sc_grav_norm']>df_france.sc_grav_norm.describe()['50%'])),'risk_gravity']=2\n",
    "#create two levels for risk_frequency : \n",
    "df_france['risk_frequence']=1\n",
    "df_france.loc[((df_france['sc_acc']<=df_france.sc_acc.describe()['50%'])),'risk_frequence']=1\n",
    "df_france.loc[((df_france['sc_acc']>df_france.sc_acc.describe()['50%'])),'risk_frequence']=2\n",
    "\n",
    "#-----------------Bretagne---------------------\n",
    "df_bretagne['sc_acc'] =(df_bretagne.nb_acc-df_bretagne.nb_acc.min())/(df_bretagne.nb_acc.max()-df_bretagne.nb_acc.min())\n",
    "# score de gravité : \n",
    "df_bretagne['sc_grav'] =( df_bretagne['indemne'] + (df_bretagne['blessé'] * np.exp(1)) +\n",
    "                                   (df_bretagne['hospitalisé'] * np.exp(2)) + (df_bretagne['tue'] * np.exp(3)))\n",
    "#normalize sc_grav\n",
    "df_bretagne['sc_grav_norm']=(df_bretagne.sc_grav)/(df_bretagne.sc_grav.max())\n",
    "# create two levels for sc_grav_norm\n",
    "df_bretagne['risk_gravity']=1\n",
    "df_bretagne.loc[((df_bretagne['sc_grav_norm']<=df_bretagne.sc_grav_norm.describe()['50%'])),'risk_gravity']=1\n",
    "df_bretagne.loc[((df_bretagne['sc_grav_norm']>df_bretagne.sc_grav_norm.describe()['50%'])),'risk_gravity']=2\n",
    "#create two levels for risk_frequency : \n",
    "df_bretagne['risk_frequence']=1\n",
    "df_bretagne.loc[((df_bretagne['sc_acc']<=df_bretagne.sc_acc.describe()['50%'])),'risk_frequence']=1\n",
    "df_bretagne.loc[((df_bretagne['sc_acc']>df_bretagne.sc_acc.describe()['50%'])),'risk_frequence']=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#last preparation : \n",
    "#-----------France ------------------------\n",
    "df_france.loc[df_france['lartpc']>10,'lartpc']=10\n",
    "df_france.loc[df_france['larrout']>74,'larrout']=74\n",
    "df_france.loc[df_france['nbv']>9,'nbv']=8\n",
    "df_france.loc[:,'larrout']=df_france.loc[:,'larrout'].replace(-81,75)\n",
    "df_france.loc[:,'larrout']=10*(df_france.larrout-df_france.larrout.min())/(df_france.larrout.max()-df_france.larrout.min())\n",
    "\n",
    "#--------------Bretagne\n",
    "df_bretagne.loc[df_bretagne['lartpc']>10,'lartpc']=10\n",
    "df_bretagne.loc[df_bretagne['larrout']>74,'larrout']=74\n",
    "df_bretagne.loc[df_bretagne['nbv']>9,'nbv']=8\n",
    "df_bretagne.loc[:,'larrout']=df_bretagne.loc[:,'larrout'].replace(-81,75)\n",
    "df_bretagne.loc[:,'larrout']=10*(df_bretagne.larrout-df_bretagne.larrout.min())/(df_bretagne.larrout.max()-df_bretagne.larrout.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save data to csv file :\n",
    "df_france.to_csv('score-analysis/df_france.csv',index=False)\n",
    "df_bretagne.to_csv('score-analysis/df_bretagne.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_france=pd.read_csv('score-analysis/df_france.csv', encoding='latin-1')\n",
    "df_bretagne=pd.read_csv('score-analysis/df_bretagne.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modélisation & Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Score de gravité :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN : KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'algorithme KNN est un classificateur robuste et polyvalent qui est souvent utilisé comme référence pour des classificateurs plus complexes tels que les réseaux neuronaux artificiels (ANN) et les SVM. Malgré sa simplicité, KNN peut surpasser les classificateurs plus puissants et est utilisé dans une variété d'applications telles que la prévision économique, la compression de données et la génétique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formalisation de l'algorithme:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commençons par établir des définitions et des notations. Nous utiliserons x pour désigner une variable indépendente (feature predictor, attribut) et y pour désigner la cible (label, class) que nous essayons de prédire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN tombe dans la famille d'algorithmes d'apprentissage supervisé. De manière informelle, cela signifie que nous recevons un ensemble de données étiqueté contenant des observations d'entraînement (x, y) et que nous souhaitons capturer la relation entre x et y. Plus formellement, notre but est d'apprendre une fonction h: X → Y qui, étant donné une observation invisible x, h(x) peut prédire avec confiance la sortie y correspondante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le classificateur KNN est également un algorithme d'apprentissage non paramétrique et basé sur l'instance (instance based)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Non-paramétrique</b> :  signifie qu'il ne fait aucune supposition explicite sur la forme fonctionnelle de h, évitant les dangers de ne pas bien modéliser la distribution sous-jacente des données. Par exemple, supposons que nos données soient hautement non gaussiennes mais le modèle d'apprentissage que nous choisissons suppose une forme gaussienne. Dans ce cas, notre algorithme ferait des prédictions extrêmement pauvres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>L'apprentissage par instance</b> : signifie que notre algorithme n'apprend pas explicitement un modèle. Au lieu de cela, il choisit de mémoriser les instances d'entraînement qui sont ensuite utilisées comme «connaissances» pour la phase de prédiction. Concrètement, cela signifie que seulement quand une requête à notre base de données est faite (c'est-à-dire quand nous lui demandons de prédire une étiquette donnée une entrée), l'algorithme utilisera-t-il les instances de formation pour trouver une réponse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KNN mathématiquement: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "voir lien ( à faire dans le rapport)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Equilibre des modalités dans la classe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data : \n",
    "df_france.risk_gravity.value_counts()/df_france.shape[0] #===> balanced data so accuracy as performance measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bretagne.risk_gravity.value_counts()/df_bretagne.shape[0] #===> balanced data so accuracy as performance measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut dire que la distribution  est la même pour la variable cible dans les deux jeu de données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Valeur de départ pour K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ne doit pas être paire\n",
    "np.sqrt(df_france.shape[0])/2\n",
    "#on choisit alors 257 qui est bien sûre impair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading library\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "col_train =[ 'agg', 'int', 'catr', 'circ', 'vosp', 'prof', 'plan', 'lartpc','larrout' ,'infra', 'situ', 'obs', 'risk_gravity']\n",
    "data_train=df_france.loc[:,col_train]\n",
    "\n",
    "# split data into target and feature data : training\n",
    "y=data_train.risk_gravity.values  # target column\n",
    "X=data_train.loc[:,data_train.columns!='risk_gravity']  # feature data\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=1)\n",
    "\n",
    "# instantiate learning model (k = 275)\n",
    "knn_257 = KNeighborsClassifier(n_neighbors=257,n_jobs=-1, metric='euclidean',weights='distance')\n",
    "\n",
    "# fitting the model\n",
    "knn_257.fit(X_train, y_train)\n",
    "\n",
    "# predict the response\n",
    "pred = knn_257.predict(X_test)\n",
    "#evaluate on train set to assist overfitting : \n",
    "y_pred_train=knn_257.predict(X_train)\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "   \n",
    "#predict on test set :\n",
    "y_pred_test=knn_257.predict(X_test)\n",
    "cm=confusion_matrix(y_test,y_pred_test)\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "#print results : \n",
    "print(\"Accuracy on train set : %.2f%%\" % (accuracy_train * 100.0))\n",
    "print(\"Accuracy on test set : %.2f%%\" % (accuracy_test * 100.0))\n",
    "print(\"Confusion matrix : \\n \",cm)\n",
    "print('Classification report : \\n ',classification_report(y_test,y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>On voit qu'il y a de l'overfitting donc nous allons chercher la  valeur optimale pour K.</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Paramétrage avec validation croisée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette section, nous allons explorer une méthode qui peut être utilisée pour régler l'hyperparamètre K.\n",
    "\n",
    "Évidemment, le meilleur K est celui qui correspond au plus faible taux d'erreur de test, supposons donc que nous effectuons des mesures répétées de l'erreur de test pour différentes valeurs de K. Par inadvertance, nous utilisons l'ensemble de test comme ensemble d'apprentissage ! Cela signifie que nous sous-estimons le taux d'erreur réel puisque notre modèle a été contraint d'adapter le test de la meilleure façon possible. Notre modèle est alors incapable de généraliser à des observations plus récentes, un processus connu sous le nom de surapprentissage (overfitting). Par conséquent, toucher l'ensemble de test est hors de question et doit seulement être fait à la toute fin de notre pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une approche alternative et plus intelligente consiste à estimer le taux d'erreurs de test en présentant un sous-ensemble de l'ensemble d'apprentissage à partir du processus d'ajustement. Ce sous-ensemble, appelé ensemble de validation, peut être utilisé pour sélectionner le niveau de flexibilité approprié de notre algorithme! Il existe différentes approches de validation qui sont utilisées dans la pratique, et nous explorerons l'une des plus populaires, la validation croisée par k-fold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](cross_validation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme on le voit sur l'image, la validation croisée k-fold (le k est totalement indépendant de K) consiste à diviser de façon aléatoire l'ensemble d'apprentissage en k groupes, ou plis, de taille approximativement égale. Le premier pli est traité comme un ensemble de validation, et la méthode est ajustée sur les plis k-1 restants. Le taux de mauvaise classification est ensuite calculé sur les observations dans le pli bloqué. Cette procédure est répétée k fois; chaque fois, un groupe d'observations différent est traité comme un ensemble de validation. Ce processus se traduit par k estimations de l'erreur de test qui sont ensuite moyennées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import  cross_val_score\n",
    "# creating odd list of K for KNN\n",
    "k_values=[k for k in range(257,268,2)]\n",
    "# empty list that will hold cv scores\n",
    "cv_scores = []\n",
    "\n",
    "# perform 10-fold cross validation\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k,n_jobs=-1, metric='euclidean')\n",
    "    scores = cross_val_score(knn, X_train, y_train, cv=10, scoring='accuracy')\n",
    "    cv_scores.append(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing to misclassification error\n",
    "MSE = [1 - x for x in cv_scores]\n",
    "\n",
    "# determining best k\n",
    "optimal_k = k_values[MSE.index(min(MSE))]\n",
    "print( \"The optimal number of neighbors is %d\" % optimal_k)\n",
    "\n",
    "# plot misclassification error vs k\n",
    "plt.plot(k_values, MSE)\n",
    "plt.xlabel('Number of Neighbors K')\n",
    "plt.ylabel('Misclassification Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Using the optimal number of K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train set : 84.16%\n",
      "Accuracy on test set : 61.17%\n",
      "Confusion matrix : \n",
      "  [[24263 16083]\n",
      " [14703 24238]]\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          1       0.62      0.60      0.61     40346\n",
      "          2       0.60      0.62      0.61     38941\n",
      "\n",
      "avg / total       0.61      0.61      0.61     79287\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# loading library\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "col_train =[ 'agg', 'int', 'catr', 'circ', 'vosp', 'prof', 'plan', 'lartpc','larrout' ,'infra', 'situ', 'obs', 'risk_gravity']\n",
    "data_train=df_france.loc[:,col_train]\n",
    "\n",
    "# split data into target and feature data : training\n",
    "y=data_train.risk_gravity.values  # target column\n",
    "X=data_train.loc[:,data_train.columns!='risk_gravity']  # feature data\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=1)\n",
    "\n",
    "# instantiate learning model (k = 265)\n",
    "knn = KNeighborsClassifier(n_neighbors=265,n_jobs=-1, metric='euclidean',weights='distance')\n",
    "\n",
    "# fitting the model\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# predict the response\n",
    "pred = knn.predict(X_test)\n",
    "#evaluate on train set to assist overfitting : \n",
    "y_pred_train=knn.predict(X_train)\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "   \n",
    "#predict on test set :\n",
    "y_pred_test=knn.predict(X_test)\n",
    "cm=confusion_matrix(y_test,y_pred_test)\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "#print results : \n",
    "print(\"Accuracy on train set : %.2f%%\" % (accuracy_train * 100.0))\n",
    "print(\"Accuracy on test set : %.2f%%\" % (accuracy_test * 100.0))\n",
    "print(\"Confusion matrix : \\n \",cm)\n",
    "print('Classification report : \\n ',classification_report(y_test,y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#knn for gravity : \n",
    "with open('knn.pkl', 'wb') as fid:\n",
    "    pickle.dump(knn, fid,2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 2. Score d'accidents : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create two levels for risk_frequency : \n",
    "df_france['risk_frequence']=1\n",
    "df_france.loc[df_france['nb_acc']<=3,'risk_frequence']=1  # accidents pas fréquents\n",
    "df_france.loc[df_france['nb_acc']>3,'risk_frequence']=2  # accidents très fréquents \n",
    "\n",
    "df_bretagne['risk_frequence']=1\n",
    "df_bretagne.loc[df_bretagne['nb_acc']<=3,'risk_frequence']=1\n",
    "df_bretagne.loc[df_bretagne['nb_acc']>3,'risk_frequence']=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### KNN with the optimal K values founded previously :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A noter que j'ai essayé au début de définir risk_frequence avec un seuil égale au moyen (moyen du nombre d'accidents) mais cela donne de l'overfitting ( 91% train et 60% test) Puis j'ai choisi le troisième quartile (75%) dont les résultats sont ci-dessus : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train set : 87.54%\n",
      "Accuracy on test set : 76.20%\n",
      "Confusion matrix : \n",
      "  [[54805  5680]\n",
      " [13190  5612]]\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          1       0.81      0.91      0.85     60485\n",
      "          2       0.50      0.30      0.37     18802\n",
      "\n",
      "avg / total       0.73      0.76      0.74     79287\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# loading library\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "col_train =[ 'agg', 'int', 'catr', 'circ', 'vosp', 'prof', 'plan', 'lartpc','larrout' ,'infra', 'situ', 'obs', 'risk_frequence']\n",
    "data_train=df_france.loc[:,col_train]\n",
    "\n",
    "# split data into target and feature data : training\n",
    "y=data_train.risk_frequence.values  # target column\n",
    "X=data_train.loc[:,data_train.columns!='risk_frequence']  # feature data\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=1)\n",
    "\n",
    "# instantiate learning model (k = 265)\n",
    "knn_frequence = KNeighborsClassifier(n_neighbors=265,n_jobs=-1, metric='euclidean',weights='distance')\n",
    "\n",
    "# fitting the model\n",
    "knn_frequence.fit(X_train, y_train)\n",
    "\n",
    "# predict the response\n",
    "pred = knn_frequence.predict(X_test)\n",
    "#evaluate on train set to assist overfitting : \n",
    "y_pred_train=knn_frequence.predict(X_train)\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "   \n",
    "#predict on test set :\n",
    "y_pred_test=knn_frequence.predict(X_test)\n",
    "cm=confusion_matrix(y_test,y_pred_test)\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "#print results : \n",
    "print(\"Accuracy on train set : %.2f%%\" % (accuracy_train * 100.0))\n",
    "print(\"Accuracy on test set : %.2f%%\" % (accuracy_test * 100.0))\n",
    "print(\"Confusion matrix : \\n \",cm)\n",
    "print('Classification report : \\n ',classification_report(y_test,y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les résultats sont beaucoup mieux par rapport au cas précedent.\n",
    "Mais on voit toujours que la classe minoritaire a des résultats incomprable avec les classe majoritaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.risk_frequence.value_counts().values[0]/data_train.shape[0],data_train.risk_frequence.value_counts().values[1]/data_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### UnderSampling :  resample method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    202073\n",
       "2     62216\n",
       "Name: risk_frequence, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_train =[ 'agg', 'int', 'catr', 'circ', 'vosp', 'prof', 'plan', 'lartpc','larrout' ,'infra', 'situ', 'obs', 'risk_frequence']\n",
    "data_train=df_france.loc[:,col_train]\n",
    "data_train.risk_frequence.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    62216\n",
       "1    62216\n",
       "Name: risk_frequence, dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate majority and minority classes\n",
    "df_majority = data_train[data_train.risk_frequence==1]\n",
    "df_minority = data_train[data_train.risk_frequence==2]\n",
    " \n",
    "# Downsample majority class\n",
    "df_majority_downsampled = resample(df_majority, \n",
    "                                 replace=False,    # sample without replacement\n",
    "                                 n_samples=62216,     # to match minority class\n",
    "                                 random_state=123) # reproducible results\n",
    " \n",
    "# Combine minority class with downsampled majority class\n",
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    " \n",
    "# Display new class counts\n",
    "df_downsampled.risk_frequence.value_counts()\n",
    "df_downsampled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train set : 86.59%\n",
      "Accuracy on test set : 63.60%\n",
      "Confusion matrix : \n",
      "  [[10277  8455]\n",
      " [ 5133 13465]]\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          1       0.67      0.55      0.60     18732\n",
      "          2       0.61      0.72      0.66     18598\n",
      "\n",
      "avg / total       0.64      0.64      0.63     37330\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# loading library\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "# split data into target and feature data : training\n",
    "y=df_downsampled.risk_frequence.values  # target column\n",
    "X=df_downsampled.loc[:,df_downsampled.columns!='risk_frequence']  # feature data\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=1)\n",
    "\n",
    "# instantiate learning model (k = 265)\n",
    "knn_frequence = KNeighborsClassifier(n_neighbors=265,n_jobs=-1, metric='euclidean',weights='distance')\n",
    "\n",
    "# fitting the model\n",
    "knn_frequence.fit(X_train, y_train)\n",
    "\n",
    "# predict the response\n",
    "pred = knn_frequence.predict(X_test)\n",
    "#evaluate on train set to assist overfitting : \n",
    "y_pred_train=knn_frequence.predict(X_train)\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "   \n",
    "#predict on test set :\n",
    "y_pred_test=knn_frequence.predict(X_test)\n",
    "cm=confusion_matrix(y_test,y_pred_test)\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "#print results : \n",
    "print(\"Accuracy on train set : %.2f%%\" % (accuracy_train * 100.0))\n",
    "print(\"Accuracy on test set : %.2f%%\" % (accuracy_test * 100.0))\n",
    "print(\"Confusion matrix : \\n \",cm)\n",
    "print('Classification report : \\n ',classification_report(y_test,y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Oversampling :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>int</th>\n",
       "      <th>catr</th>\n",
       "      <th>circ</th>\n",
       "      <th>vosp</th>\n",
       "      <th>prof</th>\n",
       "      <th>plan</th>\n",
       "      <th>lartpc</th>\n",
       "      <th>larrout</th>\n",
       "      <th>infra</th>\n",
       "      <th>situ</th>\n",
       "      <th>obs</th>\n",
       "      <th>risk_frequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.729730</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.351351</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   agg  int  catr  circ  vosp  prof  plan  lartpc    larrout  infra  situ  \\\n",
       "0    1    0   1.0   1.0   0.0   1.0   1.0    10.0   4.729730    3.0   1.0   \n",
       "2    1    0   1.0   1.0   0.0   1.0   1.0     2.0   1.351351    0.0   1.0   \n",
       "3    1    0   2.0   2.0   0.0   0.0   0.0     0.0   0.000000    0.0   1.0   \n",
       "4    1    0   2.0   3.0   0.0   1.0   1.0    10.0  10.000000    0.0   1.0   \n",
       "7    1    0   3.0   2.0   0.0   1.0   0.0     0.0   0.000000    0.0   1.0   \n",
       "\n",
       "   obs  risk_frequence  \n",
       "0  0.0               1  \n",
       "2  3.0               1  \n",
       "3  0.0               1  \n",
       "4  4.0               1  \n",
       "7  0.0               1  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate majority and minority classes\n",
    "df_majority = data_train[data_train.risk_frequence==1]\n",
    "df_minority = data_train[data_train.risk_frequence==2]\n",
    " \n",
    "# Downsample majority class\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,    # sample without replacement\n",
    "                                 n_samples=202073,     # to match majority class\n",
    "                                 random_state=123) # reproducible results\n",
    " \n",
    "# Combine minority class with downsampled majority class\n",
    "df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    " \n",
    "# Display new class counts\n",
    "df_upsampled.risk_frequence.value_counts()\n",
    "df_upsampled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train set : 85.47%\n",
      "Accuracy on test set : 72.38%\n",
      "Confusion matrix : \n",
      "  [[36095 24468]\n",
      " [ 9014 51667]]\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          1       0.80      0.60      0.68     60563\n",
      "          2       0.68      0.85      0.76     60681\n",
      "\n",
      "avg / total       0.74      0.72      0.72    121244\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# loading library\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "# split data into target and feature data : training\n",
    "y=df_upsampled.risk_frequence.values  # target column\n",
    "X=df_upsampled.loc[:,df_upsampled.columns!='risk_frequence']  # feature data\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=1)\n",
    "\n",
    "# instantiate learning model (k = 265)\n",
    "knn_frequence = KNeighborsClassifier(n_neighbors=265,n_jobs=-1, metric='euclidean',weights='distance')\n",
    "\n",
    "# fitting the model\n",
    "knn_frequence.fit(X_train, y_train)\n",
    "\n",
    "# predict the response\n",
    "pred = knn_frequence.predict(X_test)\n",
    "#evaluate on train set to assist overfitting : \n",
    "y_pred_train=knn_frequence.predict(X_train)\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "   \n",
    "#predict on test set :\n",
    "y_pred_test=knn_frequence.predict(X_test)\n",
    "cm=confusion_matrix(y_test,y_pred_test)\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "#print results : \n",
    "print(\"Accuracy on train set : %.2f%%\" % (accuracy_train * 100.0))\n",
    "print(\"Accuracy on test set : %.2f%%\" % (accuracy_test * 100.0))\n",
    "print(\"Confusion matrix : \\n \",cm)\n",
    "print('Classification report : \\n ',classification_report(y_test,y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#knn for frequence : \n",
    "with open('knn_frequence.pkl', 'wb') as fid:\n",
    "    pickle.dump(knn_frequence, fid,2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion : on garde notre algorithme avec upsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Déploiement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#reload knn for gravity : \n",
    "knn = pickle.load(open('knn.pkl', 'rb'))\n",
    "\n",
    "#reload knn_frequence for frequence\n",
    "knn_frequence = pickle.load(open('knn_frequence.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'application qui sera mise en production va permettre à un utilisateur d'entrer une  configuration d'une route puis l'application va lui rendre :\n",
    "1. un score de gravité d'accident et de fréquence avec les probabilités corresspondants\n",
    "2. trouver les plus proches routes dans la base de données \n",
    "4. proposer un aménagement possible avec la plus proche configuration et qui a un score très faible de gravité.\n",
    "\n",
    "\n",
    "<mark>Application :</mark> \n",
    "- input : [  'agg', 'int', 'catr', 'circ', 'nbv', 'vosp', 'prof', 'plan', 'lartpc', 'larrout', 'infra', 'situ', 'obs']\n",
    " \n",
    " Les inputs doievent être compréhensible par l'utilisateur, donc c'est à nous de faire le traitement des valeurs saisies pour qu'elles soient cohérentes avec les données d'apprentissage et du test\n",
    " \n",
    " \n",
    " - output :  ['risk_gravity']  ['risk_frequence'] and probabilities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_gravity_frequence(new_instance,n_voisins=265):\n",
    "    '''\n",
    "    Parameters : \n",
    "        - new_instance : \n",
    "        - n_voisins : \n",
    "    \n",
    "    \n",
    "    Returns : \n",
    "        -  two Dataframes  : \n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    #gravity : \n",
    "    prediction_gravity= knn.predict(new_instance.reshape(1, -1))\n",
    "    prediction_gravity_proba= knn.predict_proba(new_instance.reshape(1, -1) )\n",
    "    \n",
    "    #frequence : \n",
    "    prediction_frequence= knn_frequence.predict(new_instance.reshape(1, -1) )\n",
    "    prediction_frequence_proba= knn_frequence.predict_proba(new_instance.reshape(1, -1) )\n",
    "    #results into a dataframe  :\n",
    "    my_dict=new_instance.to_dict()\n",
    "    res=pd.DataFrame(my_dict,index=[0])\n",
    "    #print(prediction_gravity[0],prediction_frequence[0],prediction_gravity_proba[0][prediction_gravity-1][0],\n",
    "    #      prediction_frequence_proba[0][prediction_frequence-1][0])\n",
    "    #gravity results : \n",
    "    if prediction_gravity[0]==1 :\n",
    "         res['la gravité']='Route non dangereuse'\n",
    "    else : \n",
    "        res['la gravité']='Route très dangereuse'\n",
    "    res['probabilité de la gravité']=prediction_gravity_proba[0][prediction_gravity-1][0]\n",
    "    #frequence results :  \n",
    "    if prediction_frequence[0]==1 :\n",
    "         res['la fréquence des accidents']='accidents non fréquents'\n",
    "    else : \n",
    "        res['la fréquence des accidents']='accidents très fréquents'\n",
    "    res['probabilité de la fréquence']=prediction_frequence_proba[0][prediction_frequence-1][0]\n",
    "    \n",
    "    \n",
    "    # amenagement dans le cas où la route est très dangereuse : *voir fonction ci-dessous\n",
    "    if prediction_gravity[0]==2:\n",
    "        solution=amengament(new_instance,n_voisins)\n",
    "        \n",
    "        \n",
    "    return res,solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def amengament(instance,n_voisins=265):\n",
    "    '''\n",
    "    cette fonction propose un améngement pour une route très dangereuse.\n",
    "    Elle permet de chercher dans la base les plus proches voisin dont le risk de gravité est nulle.\n",
    "    '''\n",
    "    ###### trouvons les plus proches voisin à cette route en utilisant knn de la gravité : \n",
    "    voisins= knn.kneighbors(instance.reshape(1, -1),n_neighbors=n_voisins,return_distance=False)\n",
    "    res=pd.DataFrame()\n",
    "    for j in range(n_voisins) :\n",
    "        i=voisins[0][j] # get index of the first neighbor for the instance used for validation ( here j=0)\n",
    "        risk_road=df_france.loc[i,'risk_gravity']  # get classification of the neighbor\n",
    "        if risk_road==1:\n",
    "            my_dict=df_france.loc[i,col_train].to_dict()\n",
    "            del my_dict['risk_gravity']\n",
    "            del my_dict['risk_frequence']\n",
    "            df=pd.DataFrame(my_dict,index=[0])\n",
    "            res=pd.concat([res,df])\n",
    "    #retransforme larrout :\n",
    "    res['larrout']=(1/10)*res['larrout']*((df_bretagne.larrout.max()-df_bretagne.larrout.min()))+df_bretagne.larrout.min()\n",
    "    # create index : \n",
    "    res=res.reset_index()\n",
    "    res=res.iloc[:,1:]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test de la solution :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation data : \n",
    "col_train =[ 'agg', 'int', 'catr', 'circ', 'vosp', 'prof', 'plan', 'lartpc','larrout' ,'infra', 'situ', 'obs', 'risk_gravity','risk_frequence']\n",
    "# df_bretagne est le jeu de validation de notre solution \n",
    "validation=df_bretagne.loc[:,col_train]\n",
    "# instance à valider : on l'a choisit car elle est très dangereuse et on veut vérifier que les résultats fournis par notre solution \n",
    "# correspond bien à ce qui est dans le jeu de données.\n",
    "instance=validation.iloc[47,:-2]   # retirer les variables cibles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultas = prediction_gravity_frequence(instance,n_voisins=10)[0]\n",
    "amengament_propose = prediction_gravity_frequence(instance,n_voisins=10)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>catr</th>\n",
       "      <th>circ</th>\n",
       "      <th>infra</th>\n",
       "      <th>int</th>\n",
       "      <th>larrout</th>\n",
       "      <th>lartpc</th>\n",
       "      <th>obs</th>\n",
       "      <th>plan</th>\n",
       "      <th>prof</th>\n",
       "      <th>situ</th>\n",
       "      <th>vosp</th>\n",
       "      <th>la gravité</th>\n",
       "      <th>probabilité de la gravité</th>\n",
       "      <th>la fréquence des accidents</th>\n",
       "      <th>probabilité de la fréquence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.459459</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Route très dangereuse</td>\n",
       "      <td>1.0</td>\n",
       "      <td>accidents très fréquents</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   agg  catr  circ  infra  int   larrout  lartpc  obs  plan  prof  situ  vosp  \\\n",
       "0  1.0   1.0   3.0    0.0  1.0  9.459459     0.0  0.0   1.0   1.0   1.0   0.0   \n",
       "\n",
       "              la gravité  probabilité de la gravité  \\\n",
       "0  Route très dangereuse                        1.0   \n",
       "\n",
       "  la fréquence des accidents  probabilité de la fréquence  \n",
       "0   accidents très fréquents                          1.0  "
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# resultats \n",
    "resultas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>catr</th>\n",
       "      <th>circ</th>\n",
       "      <th>infra</th>\n",
       "      <th>int</th>\n",
       "      <th>larrout</th>\n",
       "      <th>lartpc</th>\n",
       "      <th>obs</th>\n",
       "      <th>plan</th>\n",
       "      <th>prof</th>\n",
       "      <th>situ</th>\n",
       "      <th>vosp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.378378</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   agg  catr  circ  infra  int    larrout  lartpc   obs  plan  prof  situ  \\\n",
       "0  2.0   2.0   1.0    0.0  1.0  10.000000     0.0  14.0   1.0   1.0   1.0   \n",
       "1  2.0   4.0   2.0    0.0  1.0   8.378378    10.0   0.0   3.0   1.0   1.0   \n",
       "2  1.0   3.0   2.0    0.0  9.0   0.000000    10.0   0.0   1.0   1.0   1.0   \n",
       "3  1.0   2.0   3.0    3.0  1.0   0.000000     0.0   4.0   3.0   2.0   1.0   \n",
       "\n",
       "   vosp  \n",
       "0   0.0  \n",
       "1   0.0  \n",
       "2   0.0  \n",
       "3   0.0  "
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# amengament proposé :\n",
    "amengament_propose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commentaire :\n",
    "\n",
    " - Resultats bons\n",
    " - Mais cela nécessaite encore de filtre :  par exemple si on veut pas changer la catégorie de route, ... \n",
    " \n",
    "améliorons notre solution  :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prediction_gravity_frequence(new_instance,n_voisins,***):\n",
    "    '''\n",
    "    Parameters : \n",
    "        - new_instance : \n",
    "        - n_voisins : \n",
    "    \n",
    "    \n",
    "    Returns : \n",
    "        -  two Dataframes  : \n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    #gravity : \n",
    "    prediction_gravity= knn.predict(new_instance.reshape(1, -1))\n",
    "    prediction_gravity_proba= knn.predict_proba(new_instance.reshape(1, -1) )\n",
    "    \n",
    "    #frequence : \n",
    "    prediction_frequence= knn_frequence.predict(new_instance.reshape(1, -1) )\n",
    "    prediction_frequence_proba= knn_frequence.predict_proba(new_instance.reshape(1, -1) )\n",
    "    #results into a dataframe  :\n",
    "    my_dict=new_instance.to_dict()\n",
    "    res=pd.DataFrame(my_dict,index=[0])\n",
    "    #print(prediction_gravity[0],prediction_frequence[0],prediction_gravity_proba[0][prediction_gravity-1][0],\n",
    "    #      prediction_frequence_proba[0][prediction_frequence-1][0])\n",
    "    #gravity results : \n",
    "    if prediction_gravity[0]==1 :\n",
    "         res['la gravité']='Route non dangereuse'\n",
    "    else : \n",
    "        res['la gravité']='Route très dangereuse'\n",
    "    res['probabilité de la gravité']=prediction_gravity_proba[0][prediction_gravity-1][0]\n",
    "    #frequence results :  \n",
    "    if prediction_frequence[0]==1 :\n",
    "         res['la fréquence des accidents']='accidents non fréquents'\n",
    "    else : \n",
    "        res['la fréquence des accidents']='accidents très fréquents'\n",
    "    res['probabilité de la fréquence']=prediction_frequence_proba[0][prediction_frequence-1][0]\n",
    "    \n",
    "    \n",
    "    # amenagement dans le cas où la route est très dangereuse : *voir fonction ci-dessous\n",
    "    if prediction_gravity[0]==2:\n",
    "        solution=amengament(new_instance,n_voisins)\n",
    "    \n",
    "    # filtre sur la solution : \n",
    "    \n",
    "    # il faut récuperer leurs valeurs  :\n",
    "    agg_value=instance.to_dict()['agg']\n",
    "    catr_value=instance.to_dict()['catr']\n",
    "    solution=solution.loc[((solution['agg']==agg_value)&(solution['catr']==catr_value)),:]  \n",
    "    return res,solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>catr</th>\n",
       "      <th>circ</th>\n",
       "      <th>infra</th>\n",
       "      <th>int</th>\n",
       "      <th>larrout</th>\n",
       "      <th>lartpc</th>\n",
       "      <th>obs</th>\n",
       "      <th>plan</th>\n",
       "      <th>prof</th>\n",
       "      <th>situ</th>\n",
       "      <th>vosp</th>\n",
       "      <th>la gravité</th>\n",
       "      <th>probabilité de la gravité</th>\n",
       "      <th>la fréquence des accidents</th>\n",
       "      <th>probabilité de la fréquence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.459459</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Route très dangereuse</td>\n",
       "      <td>1.0</td>\n",
       "      <td>accidents très fréquents</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   agg  catr  circ  infra  int   larrout  lartpc  obs  plan  prof  situ  vosp  \\\n",
       "0  1.0   1.0   3.0    0.0  1.0  9.459459     0.0  0.0   1.0   1.0   1.0   0.0   \n",
       "\n",
       "              la gravité  probabilité de la gravité  \\\n",
       "0  Route très dangereuse                        1.0   \n",
       "\n",
       "  la fréquence des accidents  probabilité de la fréquence  \n",
       "0   accidents très fréquents                          1.0  "
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultas = prediction_gravity_frequence(instance,n_voisins=100)[0]\n",
    "amengament_propose = prediction_gravity_frequence(instance,n_voisins=100)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reste à faire  : \n",
    "\n",
    "    1- ajouter les *** dans les paramétres pour les filtres \n",
    "    \n",
    "    2- ajouter condition sur quel critère on fait aménagement : si que l'un des score==2 on fait l'aménagement dessus \n",
    "        si les deux valent 2 alors on laisse le choix au utilisateur de choisir sur quel critére on fait l'améngament\n",
    "        \n",
    "    3- Application Flask finale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
